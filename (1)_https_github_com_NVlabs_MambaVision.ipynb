{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### The code downloads a file from a given URL and saves it locally as pretrained_weights.pth in the /content directory."
      ],
      "metadata": {
        "id": "9pS2TZcHbs7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://github.com/NVlabs/MambaVision/blob/main/pretrained_weights.pth?raw=true'\n",
        "r = requests.get(url)\n",
        "\n",
        "# Save the file in the /content directory\n",
        "file_path = '/content/pretrained_weights.pth'\n",
        "with open(file_path, 'wb') as f:\n",
        "    f.write(r.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FTMzX9mpn3br",
        "outputId": "8563a541-e041-473b-ab56-c57b78c15822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"print(f'File downloaded and saved to {file_path}')\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To tackle the conversion of the MambaVision repository from PyTorch to TensorFlow, follow the steps below. We'll start by analyzing the repository, understanding the structure, and converting the code step-by-step. This process will include commenting on each function, validating the conversion, and ensuring that the TensorFlow implementation matches the original PyTorch model.\n",
        "\n",
        "### 1. Analyze and Comment on Each Function\n",
        "Now, let's analyze the repository structure and understand each function. Here's a basic template to start with:"
      ],
      "metadata": {
        "id": "8nKTB-f0v9Uz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#'torch' is the main PyTorch library.\n",
        "import torch\n",
        "#'torch.nn' contains modules and classes for building neural networks.\n",
        "import torch.nn as nn\n",
        "#'torch.nn.functional' includes functions that are used to operate on tensors.\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "#A new class 'ExampleModel' is defined, inheriting from 'nn.Module'. This is a base class for all neural network modules in PyTorch.\n",
        "class ExampleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ExampleModel, self).__init__()\n",
        "        #3: Number of input channels (e.g., RGB channels in an image).64: Number of output channels (filters). kernel_size=3: Filters are 3x3. padding=1: Output has the same width and height as the input.\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        #64 is the number of input channels (from the previous layer's output). 128 is the number of output channels (filters). kernel_size=3 and padding=1 as before.\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        #128 * 8 * 8 is the number of input features. input images are 32x32, they are downsampled to 8x8 through two max-pooling layers. 256 is the number of output features.\n",
        "        self.fc1 = nn.Linear(128 * 8 * 8, 256)\n",
        "        #256 is the number of input features from the previous layer. 10 is the number of output features, typically corresponding to the number of classes for classification.\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #The input x is passed through conv1, followed by a ReLU activation function.\n",
        "        x = F.relu(self.conv1(x))\n",
        "        #The output from conv1 is downsampled using a 2x2 max-pooling layer.\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        #The downsampled output is passed through conv2 and a ReLU activation function.\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        #The output tensor is reshaped (flattened) into a 2D tensor, where the first dimension is the batch size, and the second dimension is the number of features (128 * 8 * 8).\n",
        "        x = x.view(x.size(0), -1)\n",
        "        #The flattened tensor is passed through the first fully connected layer fc1 followed by a ReLU activation function.\n",
        "        x = F.relu(self.fc1(x))\n",
        "        #The output from fc1 is passed through the second fully connected layer fc2.\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "DU-mP016v9f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example TensorFlow conversion\n",
        "import tensorflow as tf\n",
        "\n",
        "class ExampleModelTF(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(ExampleModelTF, self).__init__()\n",
        "        #64 filters, 3x3 kernel, same padding, input shape with 3 channels.\n",
        "        self.conv1 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', input_shape=(None, None, 3))\n",
        "        #128 filters, 3x3 kernel, same padding.\n",
        "        self.conv2 = tf.keras.layers.Conv2D(128, (3, 3), padding='same')\n",
        "        # Max pooling with 2x2 window.\n",
        "        self.pool = tf.keras.layers.MaxPooling2D((2, 2))\n",
        "        #Flatten the input tensor.\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        #256 units, ReLU activation.\n",
        "        self.fc1 = tf.keras.layers.Dense(256, activation='relu')\n",
        "        #10 units (output classes).\n",
        "        self.fc2 = tf.keras.layers.Dense(10)\n",
        "\n",
        "    def call(self, x):\n",
        "        #Apply conv1 and ReLU activation\n",
        "        x = tf.nn.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = tf.nn.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "LSgN3fvLwJLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Print Model Summary\n",
        "Print the model summary in both frameworks to validate the conversion process:"
      ],
      "metadata": {
        "id": "JdzSLNcPwPqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch model summary\n",
        "model = ExampleModel()\n",
        "print(model)\n",
        "\n",
        "# TensorFlow model summary\n",
        "model_tf = ExampleModelTF()\n",
        "# Specify the input shape (batch size, height, width, channels) and build the TensorFlow model.\n",
        "model_tf.build((None, 32, 32, 3))\n",
        "model_tf.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "S3_Qcz0ywVkb",
        "outputId": "a8dab874-98e6-4a2e-b012-0d6383d69eb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ExampleModel(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (fc1): Linear(in_features=8192, out_features=256, bias=True)\n",
            "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'example_model_tf', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"example_model_tf\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"example_model_tf\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output for both PyTorch and TensorFlow models:\n",
        "\n",
        "- **conv1:** Convolutional layer with 3 input channels, 64 output channels, 3x3 kernel, and padding of 1.\n",
        "- **conv2:** Convolutional layer with 64 input channels, 128 output channels, 3x3 kernel, and padding of 1.\n",
        "- **fc1:** Fully connected layer with 8192 input features and 256 output features.\n",
        "- **fc2:** Fully connected layer with 256 input features and 10 output features.\n",
        "\n",
        "### TensorFlow Model Summary Warning\n",
        "- **Warning:** Advises against passing `input_shape` directly to layers in functional or subclass models. Suggests using `Input(shape)` instead.\n",
        "- **Warning:** Indicates that the `build()` method was called, but the layer might not have been properly built.\n",
        "\n",
        "### TensorFlow Model Summary\n",
        "- **Layers:** Lists each layer type (Conv2D, MaxPooling2D, Flatten, Dense).\n",
        "- **Output Shape:** Not built yet, so output shapes are unknown.\n",
        "- **Param #:** Indicates the parameters are not yet calculated since the model hasn't been built properly."
      ],
      "metadata": {
        "id": "DbKPiIUgndzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Print Summary for All Models\n",
        "Iterate through all models (tiny to large) in the repository and print their summaries.\n",
        "\n",
        "### 4. Import Pretrained Weights and Perform Inference\n",
        "Load pretrained weights from PyTorch into TensorFlow and perform inference:"
      ],
      "metadata": {
        "id": "zc1vL_h0wcxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Example PyTorch Model\n",
        "class ExampleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ExampleModel, self).__init__()\n",
        "        #Define a fully connected linear layer (self.layer) with 10 input features and 10 output features. nn.Linear(10, 10) creates this layer.\n",
        "        self.layer = nn.Linear(10, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "# Create and save the PyTorch model weights\n",
        "pytorch_model = ExampleModel()\n",
        "torch.save(pytorch_model.state_dict(), '/content/model_weights.pth')"
      ],
      "metadata": {
        "id": "bdda-o60Bdp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 5. Translate Functions\n",
        "Translate necessary functions from PyTorch to TensorFlow. Skip unnecessary functions as identified:"
      ],
      "metadata": {
        "id": "I1pMLWLbwnVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the transforms module from the torchvision library, which provides common image transformations.\n",
        "import torchvision.transforms as transforms\n",
        "##Add a resize transformation to resize images to 32x32 pixels.\n",
        "transform = transforms.Compose([transforms.Resize((32, 32)),transforms.ToTensor(),])\n",
        "\n",
        "# Define a function transform_tf that takes an image as an input.\n",
        "def transform_tf(image):\n",
        "    #Resize the input image to 32x32 pixels using TensorFlow's tf.image.resize function.\n",
        "    image = tf.image.resize(image, [32, 32])\n",
        "    #Convert the resized image to a TensorFlow tensor with data type tf.float32 using tf.convert_to_tensor.\n",
        "    image = tf.convert_to_tensor(image, dtype=tf.float32)\n",
        "    return image"
      ],
      "metadata": {
        "id": "GkaSYPdHwzYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complete Translation\n",
        "Continue this process for the entire repository. Ensure that each PyTorch function has a corresponding TensorFlow implementation, and test each component thoroughly."
      ],
      "metadata": {
        "id": "E3vfudmyw36y"
      }
    }
  ]
}